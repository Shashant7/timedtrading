# Lessons Learned

> Update after ANY correction from the user.
> Review at session start. Ruthlessly iterate until mistake rate drops.

## Patterns to Avoid

- **Don't block gold_short entries with direction mismatch checks**: Gold SHORT is an intentional mean-reversion play on HTF_BULL_LTF_BULL blow-off tops. 82.7% of big DOWN moves start from BULL state. The fix should only block entries WITHOUT a recognized entry path. [2026-02-06]
- **Don't use below_trigger exit on tiny price dips**: A 0.04% dip below anchor price is noise, not a signal. Require at least 0.3% adverse move before flagging trigger invalidation. Winner median gain is 1.0%, so sub-0.3% dips are normal. [2026-02-06]
- **Don't use 30-minute recent trade window**: Too aggressive for replay and prevents re-entry after quick exits. 10 minutes is sufficient to prevent churning. [2026-02-06]
- **Always check purge-ticker cleans BOTH KV and D1**: KV trades and D1 positions/lots/events must be cleaned together, otherwise dashboard shows stale cards. [2026-02-06]

## Rules to Prevent Mistakes

- When modifying trade direction logic: verify gold_short and gold_long entry paths are preserved
- When modifying exit triggers: ensure minimum adverse move thresholds prevent noise exits
- When cleaning up trades: always purge from both KV and D1 (use /timed/debug/purge-ticker)
- After any trade simulation changes: reset + replay to verify with clean data
- **Deploy worker from `worker/` directory without `--env` flag**: Bindings (KV, D1, vars, crons) are now at top-level in wrangler.toml (not under `[env.production]`). Run `cd worker && npx wrangler deploy`. The `--env production` flag fails because there's no `[env.production]` section. [2026-02-10]
- **Keep backfill script tickers in sync with SECTOR_MAP**: Use `require('../worker/sector-mapping.js')` instead of a hardcoded list to avoid drift. [2026-02-06]
- **D1 schema migrations need fallback handling**: When adding columns (e.g., `session`) via ALTER TABLE, the throttle can prevent the migration from running. SELECT and INSERT queries must not reference the new column until migration confirms. Use a fallback INSERT without the column, and SELECT only core columns. [2026-02-07]
- **New scoring fields need backward-compatible gates**: When adding new precision scoring fields (fuel gauge, ST support, etc.) to `qualifiesForEnter()` and `classifyKanbanStage()`, always check if the data exists before gating on it. Old KV data won't have the fields. Use `d?.field != null` checks. [2026-02-07]
- **Use `enrichResult()` wrapper for cross-cutting entry enrichments**: Rather than modifying every `return { qualifies: true }` individually, use a wrapper function that applies Golden Gate boosts, precision metrics, etc. to all qualifying entries consistently. [2026-02-07]
- **Use daily candles for price performance, not trail data**: For 5D/15D/30D/90D price change calculations, use `ticker_candles` (tf='D') — actual market close prices with 400+ days of history. Trail data (timed_trail/trail_5m_facts) is for scoring snapshots, not price lookbacks. Mixing them causes gaps, dedup complexity, and inaccurate prices from intraday snapshots. [2026-02-08]
- **Alpaca uses BRK.B not BRK-B**: Alpaca API rejects `BRK-B` (HTTP 400). The correct symbol format is `BRK.B` with a dot. A single bad symbol in a batch request fails the entire batch. [2026-02-08]
- **CORS wildcard needs explicit handling**: When `CORS_ALLOW_ORIGIN = "*"`, splitting by comma gives `["*"]` which is length 1, not 0. The `allowedOrigins.includes(origin)` check doesn't match `"*"` against `"http://localhost:8765"`. Must explicitly check `allowedOrigins.includes("*")` as an early exit to `allowed = "*"`. [2026-02-08]
- **Pattern matching in hot path needs in-memory cache**: D1 queries on every ingest (270+ tickers × every minute) would be too expensive. Cache the 17 active patterns in memory with a 5-minute TTL. Pattern matching itself is synchronous and fast — it's the D1 read that needs caching. [2026-02-08]
- **Pattern integration should boost, not gate**: Pattern matching enhances Kanban decisions but never overrides them. A "pattern boost" upgrades entry confidence or promotes watch→setup, but a pattern mismatch never blocks an entry that the rule-based system qualifies. This preserves system stability while the model learns. [2026-02-08]
- **Compiled JS must match source format**: If `shared-right-rail.compiled.js` is loaded via `<script>`, it must be pre-transpiled (no JSX). If the source uses JSX, change the script tag to `type="text/babel"` so Babel standalone processes it at runtime. [2026-02-08]
- **Snap intraday candle timestamps to timeframe boundaries**: Raw candle timestamps from the DB may not align cleanly to timeframe intervals (e.g., a 10m candle at 9:33 instead of 9:30). Snap with `Math.floor(ts / intervalMs) * intervalMs` and re-aggregate duplicates. [2026-02-08]
- **Compute technical overlays client-side from candle data**: EMAs, SuperTrend, ATR — all computable from OHLC candle arrays in the browser. No need for additional API calls. EMA: seed with SMA, then apply exponential smoothing. SuperTrend: ATR-based with directional flips. [2026-02-08]
- **Right rail overlay must use opaque background**: When the right rail slides over the main content, its container must use an opaque background (e.g., `bg-[#0b0e11]`) not a transparent one (e.g., `bg-white/[0.02]`). Transparent overlays let the underlying dashboard bleed through and are unreadable. [2026-02-08]
- **Bulk sed color replacements need syntax verification**: When doing bulk `sed` replacements across large JSX files (e.g., `bg-[#161922]` → `bg-white/[0.02]`), verify no broken syntax results. Tailwind arbitrary values with `/` (opacity) inside JSX strings are valid, but context matters. Also: Babel takes longer on files >500KB — the render timeout may fire before transpilation finishes. [2026-02-08]
- **Server-side TD Sequential replaces TradingView webhook dependency**: Computing DeMark Sequential (TD9/TD13) from D/W/M candles in the worker removes the dependency on TradingView Pine Script webhooks. Alpaca's `1Month` timeframe gives accurate monthly OHLCV without needing to derive from daily candles. The `normalizeTfKey` function must handle "M" without colliding with "1M" (which maps to 1-minute). [2026-02-08]
- **Model features should include all signal sources**: When adding new indicator computations (like TD Sequential), always integrate them into the self-learning model's feature vector (`flags_json`), prediction triggers (`shouldLogPrediction`), and outcome tracking. Without this, the model can't learn from the new signals. [2026-02-08]
- **Alpaca multi-symbol `limit` is TOTAL not per-symbol**: The `/v2/stocks/bars` endpoint's `limit` param caps the TOTAL bars across ALL symbols, sorted by symbol then timestamp. With `limit=5` and 141 symbols, you get 5 bars for the first alphabetical symbol and nothing else. Use `alpacaFetchAllBars` with `limit=10000` + pagination. [2026-02-09]
- **One bad symbol fails the entire Alpaca batch**: Alpaca returns HTTP 400 for invalid symbols (futures like `ES1!`, indices like `US500`, dashes like `BRK-B`). A single bad symbol in the `symbols=` param rejects the ENTIRE request. Always filter the ticker list before Alpaca API calls. [2026-02-09]
- **D1 has 1000 subrequests per Worker invocation**: Individual `d1UpsertCandle()` calls in a loop will hit "Too many API requests" with >1000 candles. Use `db.batch()` (max 500 statements per call) to write in bulk. This applies to cron handlers AND the `/timed/ingest-candles` endpoint. [2026-02-09]
- **Every price source should build candles**: TradingView ingests, Alpaca price feed snapshots, and capture heartbeats all provide price data every ~1 min. All three should upsert 1m candles to `ticker_candles` using merge semantics (`MAX(h)`, `MIN(l)`, latest `c`). This gives triple-redundant coverage. [2026-02-09]
- **D1 batch reads save 10x subrequests in scoring crons**: Scoring 140 tickers × 9 TFs = 1260 individual D1 reads, exceeding the 1000 limit. Use `db.batch()` for reads too (not just writes): fetch all 9 TFs for one ticker in a single batch call, then pass a cache-based `getCandles` function to the scoring engine. Also batch trail writes (100+ individual → 1 batch). [2026-02-09]
- **Candle-replay trades must persist to D1, not just KV**: The scoring cron (`*/5 * * * *`) overwrites `timed:trades:all` in KV. If candle-replay only writes to KV, trades vanish within 5 minutes. Always `d1UpsertTrade()` for each batch's trades so they survive cron overwrites. [2026-02-09]
- **Set `exit_ts` on ALL exit code paths**: The main EXIT handler and the `closeTradeAtPrice` function both set `exitPrice` but neither was setting `exit_ts`. Both paths need `trade.exit_ts = asOfMs || Date.now()`. Without it, closed trades show "—" for exit date. [2026-02-09]
- **RTH entry cutoff is 3:59 PM, not 4:00 PM**: `isNyRegularMarketOpen` should use `mins < 960` (exclusive), not `mins <= 960` (inclusive). At 4:00 PM the market is closed — no new entries should be allowed at the closing bell. [2026-02-09]
- **Replay with coarse candle data produces $0 P&L "WIN" trades**: When a ticker's 5-minute candles only update every 30+ minutes (e.g., CCJ), the price stays flat from entry to exit. Entering at 9:30 and exiting at 9:55 at the same price yields $0 P&L labeled "WIN" (since `0 >= 0`). UI should show "FLAT" for ~0% P&L trades. [2026-02-09]
- **`alpacaBackfill()` must use batch D1 writes, not individual upserts**: The original implementation called `d1UpsertCandle()` per bar (one D1 round-trip each). For 9 TFs × ~500 bars = ~4500 calls, this silently exceeded D1 subrequest limits, resulting in sparse/incomplete candle data (e.g., CCJ had only 606 5m candles scattered across 8 dates). After switching to `db.batch()` with 500-statement chunks, a single ticker's backfill writes ~14K bars in ~10 seconds with 0 errors. [2026-02-09]
- **Coverage metrics must check data QUALITY, not just row count**: The ingestion-status endpoint reported CCJ as "100% coverage" because it had 606 5m candles ≥ 500 expected. But those 606 candles were scattered across 8 random dates with 6-day gaps — useless for replay. Added quality scoring that factors in: freshness (hours since latest candle), gap detection (distinct trading dates in last 30 days vs expected 21), and count. CCJ's "100% pct" became "74% quality" for 5m, correctly revealing the problem. [2026-02-09]
- **Replay must enforce the same cooldowns as live mode**: `processTradeSimulation` was using `isReplay ? {} : kvGet(...)` for `execState`, making ALL cooldowns (`ENTER_COOLDOWN_MS=4h`, `recentTradeBlocked=10m`) return `true` during replay. This caused catastrophic churn: RBLX had 11 trades in one day, PATH had 11, FSLR had 9 — all with 5-minute exit-to-re-entry gaps. Fix: pass an in-memory `execStates` Map via `replayBatchContext` that persists across intervals, mirroring the KV writes live mode uses. Also fix `recentTradeBlocked` to check `exit_ts` (not just `entry_ts`) and extend window to 30 minutes. Result: 149 trades → 89, with zero rapid re-entries. [2026-02-10]
- **Replay cleanSlate must purge D1, not just KV**: The `candle-replay` endpoint's `cleanSlate=1` only cleared KV (`timed:trades:all`, portfolio, activity feed). D1 `trades`, `trade_events`, `positions` tables were untouched. After live trading ran alongside replay data, D1 accumulated 200+ trades (89 replay + 100+ live). The simulation dashboard reads from D1, so it showed inflated metrics. Fix: add `db.batch()` DELETEs for all trade-related tables when `cleanSlate && tickerOffset === 0`. [2026-02-10]
- **$0 P&L trades should be FLAT, not WIN**: When `realizedPnl >= 0` includes `== 0`, trades that enter and exit at the same candle close price get labeled "WIN" with $0. This inflates win rate. Changed to `> 0 ? "WIN" : < 0 ? "LOSS" : "FLAT"`. Also added a flat-price exit guard: if the current price hasn't moved >0.1% from entry and the exit reason isn't SL, skip the exit and keep holding. This prevents 21 out of 126 closed trades (16.7%) from being noise. [2026-02-10]
- **pnlPct needs a fallback when trade.notional is null**: During replay, portfolio cash can run out, making `tradeNotionalFromConfidence()` return null. The `closeTradeAtPrice` then computes `pnlPct = null` since it divides by `trade.notional`. Added fallback: compute from `trade.pnl / (trade.entryPrice * trade.shares)` when notional is missing. [2026-02-10]
- **Trail data drives the model analysis scripts**: The `analyze-historical-movers.js`, `analyze-big-movers.js`, and `analyze-gold-patterns.js` scripts read from `timed_trail` (D1), not directly from `ticker_candles`. Trail data includes computed scores, states, flags, and signals per 5-min interval. After a single-day replay, only ~150 rows exist per ticker (2 days). Meaningful model recalibration requires weeks/months of trail data, built by running multi-day replays or accumulated from live scoring. [2026-02-10]
- **Trade lifecycle: frontend AND server read-time recompute must pass open position context**: The frontend kanban categoriser and the `/timed/latest` read-time recompute both called `classifyKanbanStage(data)` without open position context. This caused tickers with open trades to regress from management lanes (hold/defend/trim) to discovery lanes (setup/enter) when scoring conditions changed. The server's `/timed/all` path was correct (loaded `openPositionsMap` first). Fix: (1) Frontend override: if `tradeByTicker.has(sym) && trade.status === OPEN`, force stage to at least "hold". (2) Server `/timed/latest`: call `getPositionContext(env, ticker)` before `classifyKanbanStage`. (3) Card background: always layer `getCardSkin().bgImage` over the base, not replace it with a static gradient. [2026-02-10]